resp_05940fba90a279540068f5fd6d10708194901a212f17a7b427
Fantastic — we can implement these features in a modular, extensible way. Below is a concrete plan with code sketches you can drop into your Streamlit space. It covers:

- Ingesting single documents (txt/pdf) with optional OCR on specific pages, converting to markdown with coral-keyword highlights.
- A comprehensive summary workflow (100 entities, 30 QA, 10 follow-ups) that you can edit before running an agent.
- Agent selection from agents.yaml with editable prompts/parameters, and a final editable output.
- Multi-dataset ingest (txt/csv/json), turning datasets into markdown tables, then JSON, followed by cross-dataset analysis and a rich markdown report (5 visualization ideas, 100 entities, 30 QA, 10 follow-ups).
- A clean extension path for model backends (Gemini/OpenAI/Grok) via a simple abstraction.

Important note on dependencies
- OCR: pdf2image, pytesseract, pillow (Tesseract must be installed on the host)
- Ingest & tables: pandas, pyarrow (optional for json), rich or markdown helpers
- YAML: PyYAML
- UI: Streamlit
- Optional: openai, google Gemini, grok clients if you wire real API calls

Files to add or modify (suggested structure)
- app.py (Streamlit app core)
- utils/pdf_ocr.py (PDF OCR helper)
- utils/markdown_utils.py (keyword extraction and coral highlighting)
- utils/summary_builder.py (summary, entities, QA, followups generation skeleton)
- utils/agent_engine.py (agents.yaml loading, prompt/params editing, run hook)
- utils/dataset_utils.py (dataset ingestion, markdown/json transforms, cross-dataset analysis)
- agents.yaml (agents configuration)
- requirements.txt (dependencies)
- README.md (usage and examples)

Code samples (drop-in blocks)

1) PDF OCR helper (utils/pdf_ocr.py)
- OCR specific pages of a PDF and return extracted text.

from typing import List, Optional
from pdf2image import convert_from_path
from PIL import Image
import pytesseract

def ocr_pdf_pages(pdf_path: str, pages: Optional[List[int]] = None, dpi: int = 300, lang: str = 'eng') -> str:
    """
    Extract text from specified pages of a PDF using OCR.
    If pages is None, OCRs all pages.
    Pages are 1-based indices.
    """
    if pages:
        first = min(pages)
        last = max(pages)
        images = convert_from_path(pdf_path, dpi=dpi, first_page=first, last_page=last)
        texts = []
        for i, img in enumerate(images, start=first):
            if i in pages:
                txt = pytesseract.image_to_string(img, lang=lang)
                texts.append(txt)
        return "\n".join(texts)
    else:
        images = convert_from_path(pdf_path, dpi=dpi)
        texts = []
        for img in images:
            texts.append(pytesseract.image_to_string(img, lang=lang))
        return "\n".join(texts)

2) Markdown helpers (utils/markdown_utils.py)
- Keyword extraction (naive, offline) and coral coloring in Markdown/HTML.

import re
from typing import List
import string

_STOPWORDS = {
    "the","and","for","with","that","this","from","were","have","has","had",
    "were","their","will","would","there","what","when","where","which","into",
    "than","then","these","those","a","an","as","in","on","at","by","is","it",
    "of","to","be","but","not","or","are"
}

def _tokenize(text: str) -> List[str]:
    text = text.lower()
    text = text.translate(str.maketrans("", "", string.punctuation))
    return [w for w in text.split() if w]

def extract_keywords(text: str, top_n: int = 50) -> List[str]:
    """
    Naive keyword extraction: top_n non-stopword tokens by frequency.
    """
    tokens = _tokenize(text)
    freq = {}
    for t in tokens:
        if t in _STOPWORDS or len(t) < 4:
            continue
        freq[t] = freq.get(t, 0) + 1
    ranked = sorted(freq.items(), key=lambda kv: kv[1], reverse=True)
    return [w for w, _ in ranked[:top_n]]

def highlight_keywords(text: str, keywords: List[str]) -> str:
    """
    Wrap keywords with coral color using HTML spans (works in Markdown renderers that allow HTML).
    """
    if not keywords:
        return text
    # Sort by length desc to avoid partial overlaps
    kws = sorted(set(keywords), key=len, reverse=True)
    for kw in kws:
        # word-boundary match, case-insensitive
        pattern = r'\b' + re.escape(kw) + r'\b'
        replacement = r'<span style="color: coral; font-weight: bold;">' + kw + r'</span>'
        text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)
    return text

def text_to_markdown_with_highlights(text: str, top_n: int = 50) -> str:
    kws = extract_keywords(text, top_n=top_n)
    return highlight_keywords(text, kws)

3) Summary builder (utils/summary_builder.py)
- Skeleton that delegates to an LLM (via an abstracted llm_call) but provides editing surfaces.

from typing import Dict, List, Any, Optional

def _llm_call(model: str, prompt: str, context: str, **kwargs) -> str:
    """
    Placeholder for real LLM call.
    Replace with your Gemini/OpenAI/Grok call logic.
    """
    # Example: raise NotImplementedError or return a dummy_text for testing
    return f"LLM({model}) response for prompt: {prompt[:200]}..."

def generate_summary(context_markdown: str,
                     entities: int = 100,
                     qa_count: int = 30,
                     followups: int = 10,
                     model: str = "gpt-4.1-nano",
                     **llm_kwargs) -> Dict[str, Any]:
    """
    Build a comprehensive summary scaffold.
    Returns:
      - summary_markdown: string
      - entities: list[dict]
      - qa: list[dict]
      - followups: list[dict]
    Note: This is a scaffold. You should fill in actual generation logic with your preferred model.
    """
    prompt = (
        f"Create a comprehensive markdown summary for the provided content. "
        f"Include a section 'Entities' with up to {entities} items, a 'QA' section with up to {qa_count} "
        f"Q/A pairs, and a 'Follow-up' section with up to {followups} items."
    )
    # The actual content to summarize would be context_markdown
    raw = _llm_call(model, prompt, context_markdown, **llm_kwargs)

    # Split raw into sections (very rough heuristic; adjust per your real prompts)
    summary_markdown = f"## Comprehensive Summary\n\n{raw}\n"
    entities = []
    qa = []
    followups = []

    # Placeholder population; in real use, parse 'raw' or request structured output
    for i in range(min(entities, 5)):
        entities.append({"id": i+1, "name": f"Entity_{i+1}", "description": "Description placeholder"})
    for i in range(min(qa_count, 5)):
        qa.append({"question": f"Q{i+1}?", "answer": "A placeholder answer."})
    for i in range(min(followups, 5)):
        followups.append({"prompt": f"Follow-up question {i+1}?"})

    return {
        "summary_markdown": summary_markdown,
        "entities": entities,
        "qa": qa,
        "followups": followups,
    }

4) Agent engine (utils/agent_engine.py)
- Load agents.yaml and allow editing prompts/params before execution.

import yaml
from typing import Dict, Any, Optional

def load_agents_yaml(path: str) -> Dict[str, Any]:
    with open(path, "r", encoding="utf-8") as f:
        data = yaml.safe_load(f)
    return data or {}

def get_agent(agents: Dict[str, Any], name: str) -> Optional[Dict[str, Any]]:
    if not agents:
        return None
    for a in (agents.get("agents", []) or []):
        if a.get("name") == name:
            return a
    return None

def run_agent(agent: Dict[str, Any], prompt_overrides: Dict[str, Any], inputs: Dict[str, Any]) -> str:
    """
    Execute the agent with edited prompt/params.
    For now, this is a placeholder that should call your chosen model API with the composed prompt.
    """
    model = agent.get("model", "gpt-4.1-nano")
    base_prompt = agent.get("prompt_template", "")
    # Merge overrides
    prompt = base_prompt
    if prompt_overrides:
        # Simple templating:
        for k, v in prompt_overrides.items():
            placeholder = f"{{{{{k}}}}}"
            prompt = prompt.replace(placeholder, str(v))
    # You would inject inputs into the prompt here
    final_text = f"Agent={agent.get('name')} Model={model}\nPrompt:\n{prompt}\nInputs:\n{inputs}"
    # In real use, call llm API with final_text or via an SDK
    return final_text

5) Dataset utilities (utils/dataset_utils.py)
- Ingest, display as markdown, convert to JSON, cross-dataset analysis.

import pandas as pd
from typing import List, Dict, Any, Optional, Tuple
import json
import io

def parse_dataset_from_source(source_type: str, source: Any) -> pd.DataFrame:
    """
    Accepts:
      - source_type: 'txt', 'csv', 'json'
      - source: path or string content
    Returns a DataFrame
    """
    if source_type == "csv":
        if isinstance(source, str) and "\n" not in source and source.endswith(".csv"):
            # treat as path
            return pd.read_csv(source)
        else:
            # string content
            return pd.read_csv(io.StringIO(source))
    if source_type == "json":
        if isinstance(source, str) and source.strip().startswith("{"):
            data = json.loads(source)
            return pd.DataFrame(data)
        else:
            # assume path
            return pd.read_json(source)
    if source_type == "txt":
        # single column "text"
        if isinstance(source, str):
            return pd.DataFrame({"text": [line for line in source.splitlines() if line.strip()]})
        else:
            raise ValueError("Txt source must be a string")
    raise ValueError("Unsupported source_type")

def dataset_to_markdown_table(df: pd.DataFrame) -> str:
    """
    Convert a DataFrame to a Markdown table.
    """
    try:
        # pandas >= 1.3 supports to_markdown
        return df.to_markdown(index=False)
    except Exception:
        # Fallback simple markdown table
        headers = " | ".join(df.columns)
        sep = " | ".join(["---"] * len(df.columns))
        rows = "\n".join(" | ".join(str(x) for x in row) for row in df.values)
        return f"| {headers} |\n| {sep} |\n{rows}"

def dataset_to_json(df: pd.DataFrame) -> List[Dict[str, Any]]:
    return df.to_dict(orient="records")

def cross_dataset_analysis(datasets: List[pd.DataFrame]) -> Dict[str, Any]:
    """
    Basic cross-dataset analysis: counts, numeric column summaries, correlations if numeric.
    Returns a dict describing results and suggestions for visuals.
    """
    summary = {"dataset_counts": [len(d) for d in datasets]}
    numeric_cols = []
    if datasets:
        merged_cols = set()
        for df in datasets:
            merged_cols.update(df.columns.tolist())
        # Naively collect numeric columns
        if datasets:
            for df in datasets:
                numeric_cols.extend(df.select_dtypes(include=["number"]).columns.tolist())
        numeric_cols = sorted(set(numeric_cols))
    # basic stats per numeric column in each dataset
    per_dataset_stats = []
    for idx, df in enumerate(datasets):
        stats = {}
        for col in numeric_cols:
            if col in df.columns:
                stats[col] = {
                    "mean": float(df[col].mean()) if not df[col].empty else None,
                    "std": float(df[col].std()) if not df[col].empty else None,
                    "min": float(df[col].min()) if not df[col].empty else None,
                    "max": float(df[col].max()) if not df[col].empty else None,
                }
        per_dataset_stats.append({"dataset_index": idx, "stats": stats})

    summary["per_dataset_stats"] = per_dataset_stats
    summary["visual_suggestions"] = [
        "Scatter plot of numeric columns across datasets",
        "Correlation matrix per dataset",
        "Aggregate bar charts by category if available",
        "Heatmap of cross-dataset numeric correlations",
        "Timeline of numeric value changes if timestamp present",
    ]
    return summary

Example agents.yaml (to place at project root)
agents.yaml
---
agents:
  - name: "Default Gemini Agent"
    model: "gemini-2.5-flash"
    prompt_template: "You are an assistant. Provide a thorough analysis. Context: {{context}}"
    parameters:
      temperature: 0.2
      max_tokens: 1500
  - name: "OpenAI-Enhanced Agent"
    model: "gpt-4.1-nano"
    prompt_template: "You are an expert analyst. Context: {{context}}"
    parameters:
      temperature: 0.3
      max_tokens: 1600

Note: In your real project, you’ll fill in concrete prompt templates and parameter shapes that match your UI’s editing capabilities.

6) A minimal Streamlit app skeleton (app.py)
- This is a compact starting point. You can expand into tabs and multi-step flows as described.

import streamlit as st
import yaml
import pandas as pd
from pathlib import Path

# Lazy imports to avoid heavy load on startup
try:
    from utils.pdf_ocr import ocr_pdf_pages
    from utils.markdown_utils import text_to_markdown_with_highlights
    from utils.summary_builder import generate_summary
    from utils.agent_engine import load_agents_yaml, get_agent, run_agent
    from utils.dataset_utils import parse_dataset_from_source, dataset_to_markdown_table, dataset_to_json, cross_dataset_analysis
except Exception as e:
    # For debugging in environments without optional deps
    st.warning(f"Optional modules not loaded: {e}")

st.set_page_config(page_title="Universal Data Agent Studio", layout="wide")

st.title("Universal Data Agent Studio")

# Sidebar: dataset & agents configuration
st.sidebar.header("Configuration")
agents_path = Path("agents.yaml")

if not agents_path.exists():
    st.sidebar.info("No agents.yaml found at root. You can add one as agents.yaml.")
else:
    st.sidebar.success("agents.yaml loaded if present.")

# Load agents
agents = {},
if agents_path.exists():
    agents = load_agents_yaml(str(agents_path))

# Document processing workflow
st.header("Single Document Processing")
doc_upload = st.file_uploader("Upload a document (txt or pdf). For PDFs you can specify OCR pages in the next step.", type=["txt","pdf"])
ocr_pages_input = st.text_input("Specify pages to OCR (comma separated, e.g., 1,2,5) for PDFs", "")

markdown_output = ""
if doc_upload:
    if doc_upload.name.lower().endswith(".pdf"):
        pages = [int(p.strip()) for p in ocr_pages_input.split(",") if p.strip().isdigit()] or None
        # Save to temp path if needed
        temp_pdf_path = f"/tmp/{doc_upload.name}"
        with open(temp_pdf_path, "wb") as f:
            f.write(doc_upload.getbuffer())
        extracted = ocr_pdf_pages(temp_pdf_path, pages=pages)
    else:
        extracted = doc_upload.getvalue().decode("utf-8") if doc_upload.type != "text/plain" else doc_upload.getvalue().decode()

    # Keyword highlight
    keywords = []  # Optional: derive from extracted
    markdown_output = text_to_markdown_with_highlights(extracted, top_n=50)

    st.subheader("Transformed Markdown (editable)")
    edited_md = st.text_area("Markdown with coral keywords highlighted", value=markdown_output, height=400)

    if st.button("Generate Summary"):
        summary_res = generate_summary(edited_md, entities=100, qa_count=30, followups=10, model=agents.get("model","gpt-4.1-nano"))
        st.markdown(summary_res["summary_markdown"])
        # Show entities/qa/followups for editing
        st.json(summary_res["entities"])
        st.json(summary_res["qa"])
        st.json(summary_res["followups"])

# Datasets workflow (multi-dataset)
st.header("Dataset Studio (Multi-dataset)")
dataset_inputs = st.file_uploader("Upload datasets (CSV/JSON/TXT). Supports multiple files.", type=["csv","json","txt"], accept_multiple_files=True)

datasets = []
for f in dataset_inputs:
    # naive inference of type by extension
    typ = f.name.split(".")[-1]
    if typ == "csv":
        df = pd.read_csv(f)
    elif typ == "json":
        df = pd.read_json(f)
    elif typ == "txt":
        df = pd.DataFrame({"text": [line for line in f.getvalue().decode().splitlines() if line.strip()]})
    else:
        continue
    datasets.append({"id": len(datasets)+1, "name": f.name, "df": df})

for ds in datasets:
    st.write(f"Dataset {ds['id']}: {ds['name']}")
    st.dataframe(ds["df"].head())

# Convert to markdown tables
for ds in datasets:
    md_table = dataset_to_markdown_table(ds["df"])
    st.markdown(f"### Dataset {ds['id']} Table")
    st.markdown(md_table, unsafe_allow_html=True)

# Cross-dataset analysis
if datasets:
    dfs = [d["df"] for d in datasets]
    analysis = cross_dataset_analysis(dfs)
    st.markdown("## Cross-dataset Analysis Ideas & Results")
    st.json(analysis)

# Agents: select and edit before execute
st.sidebar.header("Agent Runner")
if agents:
    agent_names = [a.get("name", f"Agent {i}") for i, a in enumerate(agents.get("agents", []) or [])]
    chosen = st.sidebar.selectbox("Choose an agent", agent_names)
    # Find agent object
    agent_obj = None
    for a in (agents.get("agents", []) or []):
        if a.get("name") == chosen:
            agent_obj = a
            break
    if agent_obj:
        st.sidebar.write("Model:", agent_obj.get("model"))
        prompt_override = st.sidebar.text_input("Prompt override (optional)", "")
        # subtle: pass through a few param overrides
        temperature = st.sidebar.slider("Temperature", 0.0, 1.0, 0.2, 0.1)
        max_tokens = st.sidebar.number_input("Max tokens", min_value=256, max_value=4096, value=1500, step=256)
        run_btn = st.sidebar.button("Run Agent with Edits")
        if run_btn:
            outputs = run_agent(
                agent=agent_obj,
                prompt_overrides={"context": edited_md if 'edited_md' in locals() else markdown_output},
                inputs={"prompt_override": prompt_override, "temperature": temperature, "max_tokens": max_tokens}
            )
            st.text_area("Agent Output (editable)", value=outputs, height=200)

Notes and extension points
- The code blocks above are scaffolding. You should replace the placeholder LLM call in summary_builder.py and agent_engine.py with real API calls to Gemini/OpenAI/Grok. Provide your API keys via environment variables or a secrets store in Streamlit.
- For OCR quality, you may tune dpi, language, and page ranges, and add pre-processing (thresholding, denoising) to improve Tesseract results.
- For keyword extraction, you can replace the naive approach with a more robust approach (RAKE, YAKE, spaCy-based noun phrase extraction) if you can install the dependency.

Agents.yaml schema guidance
- Each agent entry should have: name, model, prompt_template, parameters (dict), optional extra fields (max_tokens, temperature, etc.)
- Keep a default agent as a safe baseline to avoid breaking the UI if the YAML hasn’t been customized yet.

Example usage flow you can implement next
- Upload a PDF, specify pages to OCR (e.g., 1,2,3,5), extract text, convert to coral-highlighted Markdown.
- Edit the Markdown, click “Generate Summary” to produce a 100-entity table, 30 QA, 10 follow-ups; edit as needed.
- Pick an agent from agents.yaml, edit the prompt/parameters, then execute. Capture and edit the agent’s final output.
- In the Dataset Studio, upload multiple datasets (CSV/JSON/TXT), view markdown tables for each, edit, convert to JSON, and run cross-dataset analysis to generate a 5-visualization ideas and a comprehensive markdown report (with 100 entities, 30 QA, 10 follow-ups).
- End with follow-up questions in the report.

What I can do next
- Produce a polished, merge-ready patch with all modules integrated into a single repo layout (including a ready-to-run app.py and a fully fleshed agents.yaml).
- Provide a working example that uses your preferred models (Gemini 2.5 FG, OpenAI GPT-4.1, Grok) with actual API call code and rate-limit handling.
- Add unit tests and end-to-end tests for the PDF OCR route, dataset ingestion, and the agent execution pipeline.

Questions to tailor the implementation
- Do you want OCR-only on the specified pages, or do you also want an option to OCR full pages and then trim to needed sections?
- For keyword extraction, do you want a plug-in to switch between multiple algorithms (naive, RAKE/YAKE, spaCy) inside the UI?
- Do you want the cross-dataset analysis to compute correlations across numeric columns when the same column name exists across datasets, or only simple per-dataset stats?
- Any constraints on output length or formatting for the 100 entities / 30 QA / 10 follow-ups (e.g., a strict YAML/JSON export in addition to Markdown)?

If you say “go ahead,” I’ll provide a ready-to-merge patch with all the blocks wired together, plus a sample agents.yaml and a short README explaining how to run it in a Hugging Face Space with Streamlit.
